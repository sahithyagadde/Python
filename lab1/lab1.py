# -*- coding: utf-8 -*-
"""lab1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TBrGifzlYIbpr6tJx8PLTpZX5oqhNMZI
"""

#4
import requests
from bs4 import BeautifulSoup

URL = 'https://catalog.umkc.edu/course-offerings/graduate/comp-sci/'
page = requests.get(URL)

soup = BeautifulSoup(page.content, 'html.parser')
myTitles = soup.find_all("span", class_="title")
myDesc = soup.find_all("p", class_="courseblockdesc")

print("number fo records fetched: " )
print("Titles: ", myTitles.__len__())
print("Descriptions: ", myDesc.__len__())
print("Records: ")
print("")
for elements in range(0, len(myTitles)):
    print(elements + 1)
    print(myTitles[elements].text)
    print(myDesc[elements].text)
    print("_" * 50)

#5
import pandas as pd
import numpy as np

#Loading the data from adult Dataset
adu1t = pd.read_csv('adult.csv')
data__train = adu1t.drop("income",axis=1)
labe1 = adu1t['income']

#Finding the Values which are missing
print("Number of missing values:\n", format(adu1t.isnull().sum()))

#Eliminate NAN values
adu1t.dropna(axis = 0, inplace= True)

#Encode the categorial features
data__binary = pd.get_dummies(adu1t) 

from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test = train_test_split(data__binary,labe1)
performance = []

# creating the Gaussian Naive Bayes object
from sklearn.naive_bayes import GaussianNB

gnb = GaussianNB()

# Training the Model
gnb.fit(x_train,y_train)
tra1n__score = gnb.score(x_train,y_train)
# Predicting the Output
test__score = gnb.score(x_test,y_test)
print(f'The result of Gaussian Naive Bayes is: Training score - {tra1n__score} - Test score - {test__score}')

performance.append({'algorithm':'Gaussian Naive Bayes', 'training_score':tra1n__score, 'testing_score':test__score})

# creating the KNN object

from sklearn.neighbors import KNeighborsClassifier

knn = KNeighborsClassifier(n_neighbors=5)
knn.fit(x_train,y_train)

knn.score(x_train,y_train)

tra1n__score = knn.score(x_train,y_train)
test__score = knn.score(x_test,y_test)

print(f'The result of K Neighbors is: Training score - {tra1n__score} - Test score - {test__score}')

performance.append({'algorithm':'K Neighbors', 'training_score':tra1n__score, 'testing_score':test__score})

# creating the SVM object 

from sklearn import svm

svc = svm.SVC(kernel='linear')


from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()

scaler.fit(data__binary,labe1)

x_train_scaled = scaler.transform(x_train)
x_test_scaled = scaler.transform(x_test)
svc.fit(x_train_scaled,y_train)

tra1n__score = svc.score(x_train_scaled,y_train)
test__score = svc.score(x_test_scaled, y_test)

print(f'The result of SVM is: Training score - {tra1n__score} - Test score - {test__score}')

performance.append({'algorithm':'SVM', 'training_score':tra1n__score, 'testing_score':test__score})

#7
import nltk
from nltk.stem import WordNetLemmatizer

#Open file input.txt in read mode
inputfi1e = open("input.txt","r")

#open file output.txt in write mode in order to store the tokenized words
with open("output.txt","w") as t:
#read each sentence in the file
    for sentence in inputfi1e:
#Perform word tokenization on the data stored in the file
        wrdtokens = nltk.word_tokenize(sentence)
#Writing it to a file
        for w in wrdtokens:
            t.write(str("\n"))
            t.write(str(w))

# creat an object for lemmatization
lemmatizer = WordNetLemmatizer()
#Open a new file outputlemmatize in write mode
with open("outputlemmatize.txt", "w") as l:
#Open the file which contains tokenized words
    w = open("output.txt","r")
    for words in w:
#Perform 1emmatization on tokenized words stored in a file
        le = lemmatizer.lemmatize(words)
        l.write(str(le))

a = open("input.txt","r")
with open("outputtrigram.txt","w") as tri:
    for sentence in a:
    #perform trigram on input.txt file
            trigram = nltk.trigrams(sentence.split())

        #store the trigrams in a file
            for ti in trigram:
                tri.write(str("\n"))
                tri.write(str(ti))

f1 = open("input.txt", "r")
readfi1e = f1.read()

tg = []
word__tokens = nltk.word_tokenize(readfi1e)
for t in nltk.ngrams(word__tokens, 3):
    tg.append(t)

wordfrequency = nltk.FreqDist(tg)
mostCommon = wordfrequency.most_common()

# Extracting the top 10 repeated trigrams.
Top__ten__trigrams = wordfrequency.most_common(10)
print("Top 10 Trigrams:\n", Top__ten__trigrams, "\n")



#  Finding all the sentences which have the most repeated tri-grams
#  Extracting the sentences and concatenating them
#  Printing the concatenated result

sent__tokens = nltk.sent_tokenize(readfi1e)
concat__resu1t = []
# Iterating the Sentences
for s in sent__tokens:
    # Iterating all the trigrams
    for a, b, c in tg:
        #iterating the top 10 trigrams from all the trigrams
        for ((p, q, r), length) in wordfrequency.most_common(20): # Comparing each of them with the top 10 trigrams
            if(a, b, c == p, q, r):
                concat__resu1t.append(s)

print("Concatenated Array: \n", concat__resu1t)
print("Maximum of Concatenated Array: \n ", max(concat__resu1t))

#8
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder
import seaborn as sns; sns.set(color_codes=True)

tra1n = pd.read_csv('College.csv')


## Nu11 values
nu11s = pd.DataFrame(tra1n.isnull().sum().sort_values(ascending=False)[:25])
nu11s.columns = ['Null Count']
nu11s.index.name = 'Feature'
print(nu11s)

## Replace nu11 values with mean values
modifieddata = tra1n.select_dtypes(include=[np.number]).interpolate().dropna()


#Use Pearson Corre1ation and p1oting the heat map
plt.figure(figsize=(20,20))
corre = modifieddata.corr()
sns.heatmap(corre, annot=True, cmap=plt.cm.Reds)
plt.show()

# Print the corre1ation with the target feature "qua1ity"
print(corre['perc.alumni'].sort_values(ascending=False)[:5],'\n')

##Bui1ding a mu1tip1e 1inear regression mode1
y = modifieddata['perc.alumni']
X = modifieddata.drop(['perc.alumni'],axis =1)

print(X.shape)

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(
                                    X, y, random_state=42, test_size=.20)
from sklearn import linear_model
lr = linear_model.LinearRegression()
model = lr.fit(X_train, y_train)
##Eva1uating the performance and visua1izing resu1ts
print ("R^2 is: \n", model.score(X_test, y_test))
pred1ct1on = model.predict(X_test)
from sklearn.metrics import mean_squared_error
print ('RMSE is: \n', mean_squared_error(y_test, pred1ct1on))

actual_values = y_test
plt.scatter(pred1ct1on, actual_values, alpha=.75,
            color='b') #alpha helps to show overlapping data
plt.xlabel('Predicted ')
plt.ylabel('Actual')
plt.title('Linear Regression Model')
plt.show()

#2
# TODO
# Author: Team 8
# Usecase: concatenating two dictionaries
import operator
from collections import Counter


def main():
    dict1 = {
        'Randomness': 5,
        'Makes': 7,
        'Life': 8,
        'Better': 8,
        'Though!': 2
    }
    dict2 = {
        'Better': 8,
        "Call": 20,
        'Soul': 7,
        'Though!': 1
    }
    # This way, The value of these words will be increased,we thought
    #  this is a better way to represent the result of this method
    myDict = ToConc(dict1, dict2)
    print(myDict)


def ToConc(dict1, dict2):
    # Converting the two to a Counter type
    d1 = Counter(dict1)
    d2 = Counter(dict2)
    toReturn = d1 + d2

    # Calling dict() since the output of this method is in counter type (subclass of dictionary)
    return dict(sorted(toReturn.items(), key=operator.itemgetter(1), reverse=True))


if __name__ == '__main__':
    main()

#3
# TODO Airplane System with classes, inheritance, and overriding techniques
# Sleep method is used widely for demonstration
# Author: Team 8
# Usecase: Airlines system using Python3

from time import sleep


class Airplane:
    def __init__(self, Model_Number, rgsNo, capacity):
        self.modelNumber = Model_Number
        self.rgsNo = rgsNo
        self.capacity = capacity

    def printInfo(self):
        print("Model Number: ", self.modelNumber)
        sleep(1)
        print("Registration Number: ", self.rgsNo)
        sleep(1)
        print("Capacity: ", self.capacity)
        sleep(1)


class Flight(Airplane):
    def __init__(self, Model_Number, rgsNo, capacity, From, To, DepDate, DepTime, ArrDate, ArrTime):
        super().__init__(Model_Number, rgsNo, capacity)
        self.From = From
        self.To = To
        self.DepDate = DepDate
        self.DepTime = DepTime
        self.ArrDate = ArrDate
        self.ArrTime = ArrTime

    def printInfo(self):
        super().printInfo()
        print("Flying from: ", self.From)
        sleep(1)
        print("Flying To: ", self.To)
        sleep(1)
        print("DepDate: ", self.DepDate)
        sleep(1)
        print("DepTime: ", self.DepTime)
        sleep(1)
        print("ArrDate: ", self.ArrDate)
        sleep(1)
        print("ArrTime: ", self.ArrTime)
        sleep(1)


class Person:
    def __init__(self, FirstName, LastName, age):
        self.FirstName = FirstName
        self.LastName = LastName
        self.age = age

    def FullName(self):
        print(self.FirstName, self.LastName)


class Employee(Person):

    def __init__(self, FirstName, LastName, age):
        super().__init__(FirstName, LastName, age)

    def FullName(self):
        super().FullName()

    def isEmployee(self):
        return True


class Passenger(Person):

    def __init__(self, FirstName, LastName, age):
        super().__init__(FirstName, LastName, age)

    def FullName(self):
        super().FullName()

    def isPassenger(self):
        return True

    def flightRegistered(self, flight=None):
        self.bookedFlight = flight


def main():
    person = Person("Abdulmuhaymin", "Zakari", 26)
    employee = Employee("Roa", "Jamjom", 43)
    passenger = Passenger("Marcelock", "Tawabeer", 47)

    print("Person Instance: ")
    person.FullName()
    sleep(2)

    print("Employee Instance: ")
    employee.FullName()
    print("Is Employee? ", employee.isEmployee())

    sleep(2)

    print("Passenger Instance: ")
    passenger.FullName()
    print("Is Passenger? ", passenger.isPassenger())
    sleep(2)

    print("is Roa an employee? ", employee.isEmployee())
    print("Let's make a flight!")
    model = input("Enter the flight Model Number: ")
    rgsNo = input("Enter the Registration Number: ")
    Capacity = int(input("Capacity of the Flight: "))
    Flyingfrom = input("Flying from: ")
    FlyingTo = input("Flying To: ")
    DepDate = input("Departure Date: ")
    DepTime = input("DepTime ")
    ArrDate = input("ArrDate: ")
    ArrTime = input("ArrTime:  ")

    flight1 = Flight(model, rgsNo, Capacity, Flyingfrom, FlyingTo, DepDate, DepTime, ArrDate, ArrTime)

    # flight1 = Flight(765238, 2362567, 56, "Canada", "KSA", "2020, 5, 17", "10:30 pm",
    #                  "2020, 5, 17", "10:32 am")

    print("Flight initialized with the following properties: ")
    sleep(1)
    flight1.printInfo()
    ans = input("You want to book this flight for The passenger '" + passenger.FirstName + "'? (y/n) ")
    if (ans == 'y') or (ans == 'Y'):
        flight1.capacity = flight1.capacity - 1;
        passenger.bookedFlight = flight1.modelNumber
        print("Success! you are registered for the Flight: ", flight1.modelNumber, " on ", flight1.DepDate)
        sleep(1)
        print("Flight Capacity is now ", flight1.capacity)
    else:
        print("OK")


if __name__ == '__main__':
    main()

#6
import pandas as pd
import numpy as np
from sklearn.decomposition import PCA
from sklearn.preprocessing import LabelEncoder, StandardScaler
import matplotlib.pyplot as plt
import seaborn as sns
sns.set(style="white", color_codes=True)
import warnings
warnings.filterwarnings("ignore")

#Loading the data from College Dataset
dataset = pd.read_csv('College.csv')
print(dataset.dtypes)


# sp1ittin the c1ass and features
x = dataset.iloc[:,[1,16]]
y = dataset.iloc[:,-1]
print(x.shape, y.shape)

print(dataset["Terminal"].value_counts())

## Print the count of Nu11 va1ues
nu11 = pd.DataFrame(x.isnull().sum().sort_values(ascending=False)[:25])
nu11.columns = ['Null Count']
nu11.index.name = 'Feature'
print(nu11)

# Plot scatter graph for a11 the co1umns
sns.FacetGrid(dataset, hue="Terminal", size=4).map(plt.scatter, "S.F.Ratio", "Expend").add_legend()
sns.FacetGrid(dataset, hue="Terminal", size=4).map(plt.scatter, "Apps", "Accept").add_legend()
plt.show()


## Replace nu11 va1ues with mean values
x = x.select_dtypes(include=[np.number]).interpolate().dropna()

# Standardize the dataset
from sklearn import preprocessing
sca1er = preprocessing.StandardScaler()
sca1er.fit(x)
X__sca1ed__array = sca1er.transform(x)
X__sca1ed = pd.DataFrame(X__sca1ed__array, columns = x.columns)

# Apply K-Means on the dataset
from sklearn.cluster import KMeans
nc1usters = 2 
km = KMeans(n_clusters=nc1usters)
km.fit(x)

# predict the cluster for each data point
y_c1uster_kmeans = km.predict(x)
from sklearn import metrics
scores = metrics.silhouette_score(x, y_c1uster_kmeans)
print("Silhoutte Score: " + str(scores))

#e1bow method 
wcss = []
for i in range(1,7):
     kmeans = KMeans(n_clusters=i,init='k-means++',max_iter=300,n_init=10,random_state=0)
     kmeans.fit(x)
     wcss.append(kmeans.inertia_)

plt.plot(range(1,7),wcss)
plt.title('Elbow Method')
plt.xlabel('Number of Clusters')
plt.ylabel('Wcss')
plt.show()

#1
# TODO
# Author: Team 8
# Usecase: Printing all possible subsets excluding null values

def SubsetsOf(MyList):
    # representing the algorithm using python
    elements = len(MyList)
    toReturn = []
    count = pow(2, elements)
    for i in range(1, count):
        temp = ""
        print("", end="")
        for j in range(0, elements):
            if (i & (1 << j)) > 0:
                temp += (str(MyList[j]) + ",")
        if temp.endswith(","):  # basically to remove the comma from the temp string
            temp = temp[:-1]
        listTemp = list(temp.split(","))  # taking back the String to a list
        if listTemp not in toReturn:  # Add to the returned list if not exist
            toReturn.append(listTemp)
    return toReturn
def main():
    myList = SubsetsOf([1, 2, 2])  # The problem given in the lab task
    print(myList)


if __name__ == '__main__':
    main()